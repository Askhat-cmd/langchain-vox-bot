from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from dotenv import load_dotenv
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging
import json
import os
import redis
import hashlib
import time
from typing import List

class CachedOpenAIEmbeddings(OpenAIEmbeddings):
    """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ OpenAI Embeddings –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, redis_client, **kwargs):
        super().__init__(**kwargs)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–∫—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à-–¥–∞–Ω–Ω—ã—Ö
        self._cache_data = {
            'redis_client': redis_client,
            'cache_prefix': "embedding_cache:",
            'cache_ttl': 3600 * 24 * 7  # 7 –¥–Ω–µ–π
        }
        
    def _get_cache_key(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫–µ—à–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        return f"{self._cache_data['cache_prefix']}{text_hash}"
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        results = []
        texts_to_embed = []
        text_indices = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        for i, text in enumerate(texts):
            cache_key = self._get_cache_key(text)
            cached_embedding = self._cache_data['redis_client'].get(cache_key)
            
            if cached_embedding:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º embedding –∏–∑ –∫–µ—à–∞
                embedding = json.loads(cached_embedding)
                results.append(embedding)
                logger.debug(f"‚úÖ Embedding –∏–∑ –∫–µ—à–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {i}")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embedding
                texts_to_embed.append(text)
                text_indices.append(i)
                results.append(None)  # –ó–∞–≥–ª—É—à–∫–∞
        
        # –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–µ—à–µ
        if texts_to_embed:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–µ–º {len(texts_to_embed)} –Ω–æ–≤—ã—Ö embeddings")
            new_embeddings = super().embed_documents(texts_to_embed)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for i, (text, embedding) in enumerate(zip(texts_to_embed, new_embeddings)):
                cache_key = self._get_cache_key(text)
                self._cache_data['redis_client'].setex(cache_key, self._cache_data['cache_ttl'], json.dumps(embedding))
                
                # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                original_index = text_indices[i]
                results[original_index] = embedding
                logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–µ—à embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {original_index}")
        
        return results
    
    def embed_query(self, text: str) -> List[float]:
        """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        cache_key = self._get_cache_key(text)
        cached_embedding = self._cache_data['redis_client'].get(cache_key)
        
        if cached_embedding:
            logger.debug("‚úÖ Embedding –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –∫–µ—à–∞")
            return json.loads(cached_embedding)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π embedding
        logger.debug("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
        embedding = super().embed_query(text)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        self._cache_data['redis_client'].setex(cache_key, self._cache_data['cache_ttl'], json.dumps(embedding))
        logger.debug("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–µ—à embedding –∑–∞–ø—Ä–æ—Å–∞")
        
        return embedding

load_dotenv()

logger = logging.getLogger(__name__)

class Agent:
    def __init__(self) -> None:
        logger.info("--- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ê–≥–µ–Ω—Ç–∞ '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' ---")
        # –¢–µ–∫—É—â–∞—è LLM –∏ –ª–µ–Ω–∏–≤—ã–π fallback
        self.llm = self._create_llm_from_env(primary=True)
        self.fallback_llm = None
        self._fallback_chains_built = False
        self.store = {}
        self.last_kb = "general"
        
        # –ù–û–í–û–ï: Redis –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è embeddings
        try:
            self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
            self.redis_client.ping()  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            self.cache_enabled = True
            logger.info("‚úÖ Redis –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ")
        except Exception as e:
            self.redis_client = None
            self.cache_enabled = False
            logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ: {e}")
        try:
            self.prompts = self.load_prompts()
        except (ValueError, FileNotFoundError, json.JSONDecodeError) as e:
            logger.critical(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤. {e}", exc_info=True)
            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∑–∞–ø—É—Å–∫ —Å –Ω–µ–≤–µ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            raise SystemExit(f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {e}")

        self._initialize_rag_chain()
        logger.info("--- –ê–≥–µ–Ω—Ç '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ---")

    def _get_cache_key(self, text: str, kb: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫–µ—à–∞ –¥–ª—è embedding –∑–∞–ø—Ä–æ—Å–∞."""
        combined = f"{text}:{kb}:{self.last_kb}"
        return f"emb:{hashlib.md5(combined.encode()).hexdigest()}"
    
    def _get_cached_documents(self, text: str, kb: str):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Redis."""
        if not self.cache_enabled:
            return None
        
        cache_key = self._get_cache_key(text, kb)
        try:
            cached = self.redis_client.get(cache_key)
            if cached:
                logger.info(f"üéØ –ö–ï–®–ò–†–û–í–ê–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è: {text[:30]}...")
                return json.loads(cached)
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
        return None
    
    def _cache_documents(self, text: str, kb: str, documents):
        """–ö–µ—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ Redis."""
        if not self.cache_enabled or not documents:
            return
            
        cache_key = self._get_cache_key(text, kb)
        try:
            # –ö–µ—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
            doc_contents = [{"content": doc.page_content, "metadata": doc.metadata} for doc in documents]
            self.redis_client.setex(
                cache_key, 
                3600,  # 1 —á–∞—Å TTL
                json.dumps(doc_contents)
            )
            logger.debug(f"üì¶ –ö–ï–®–ò–†–û–í–ê–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–µ—à")
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫–µ—à: {e}")

    def _create_llm_from_env(self, primary: bool) -> ChatOpenAI:
        """–°–æ–∑–¥–∞—ë—Ç LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.
        primary=True ‚Üí LLM_MODEL_PRIMARY, –∏–Ω–∞—á–µ LLM_MODEL_FALLBACK.
        """
        model_env_key = "LLM_MODEL_PRIMARY" if primary else "LLM_MODEL_FALLBACK"
        model_name = os.getenv(model_env_key, os.getenv("LLM_MODEL_PRIMARY", "gpt-4o-mini"))
        try:
            temperature = float(os.getenv("LLM_TEMPERATURE", "0.2"))
        except ValueError:
            temperature = 0.2
        logger.info(f"LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'PRIMARY' if primary else 'FALLBACK'} model='{model_name}', temperature={temperature}")
        return ChatOpenAI(
            model_name=model_name, 
            temperature=temperature, 
            streaming=True,
            request_timeout=15,  # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π timeout –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
            max_retries=1        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –º–∏–Ω–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–æ–≤
        )

    def load_prompts(self):
        prompts_file = os.getenv("PROMPTS_FILE_PATH")
        if not prompts_file:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PROMPTS_FILE_PATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: '{prompts_file}'")
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {prompts_file}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ {prompts_file}: {e}")
            raise

    def _initialize_rag_chain(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç/–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ RAG-—Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π LLM."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ RAG-—Ü–µ–ø–æ—á–∫–∏...")

        persist_directory = os.getenv("PERSIST_DIRECTORY")
        if not persist_directory:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PERSIST_DIRECTORY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –≤ '{persist_directory}'...")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∑–¥–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings
        if self.cache_enabled:
            embeddings = CachedOpenAIEmbeddings(
                redis_client=self.redis_client,
                chunk_size=1000
            )
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings")
        else:
            embeddings = OpenAIEmbeddings(chunk_size=1000)
            logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ embeddings (–∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ)")
            
        self.db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        kb_k = int(os.getenv("KB_TOP_K", "3"))
        self.retriever_general = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "general"}}
        )
        self.retriever_tech = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "tech"}}
        )
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ.")

        # –°—Ç—Ä–æ–∏–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π LLM
        self._build_chains_for_llm(self.llm)
        # –°–±—Ä–æ—Å–∏–º –∫—ç—à —Ñ–æ–ª–±—ç–∫–∞
        self._fallback_chains_built = False
        self.fallback_llm = None
        logger.info("--- RAG-—Ü–µ–ø–æ—á–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞ ---")

    def _build_chains_for_llm(self, llm: ChatOpenAI):
        """–°—Ç—Ä–æ–∏—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π LLM."""
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–±–∏—Ä–∞–µ–º history_aware_retriever –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        # –í–º–µ—Å—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ LLM –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ retriever'—ã
        logger.info("üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ retriever'—ã –±–µ–∑ history_aware –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ retriever'—ã –≤–º–µ—Å—Ç–æ history_aware
        rag_chain_general = create_retrieval_chain(self.retriever_general, question_answer_chain)
        rag_chain_tech = create_retrieval_chain(self.retriever_tech, question_answer_chain)

        self.conversational_rag_chain_general = RunnableWithMessageHistory(
            rag_chain_general,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        self.conversational_rag_chain_tech = RunnableWithMessageHistory(
            rag_chain_tech,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

    def _route_kb(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: general | tech.
        –í—ã–±–∏—Ä–∞–µ–º 'tech', –µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã; –∏–Ω–∞—á–µ 'general'."""
        if not text:
            return "general"
        t = text.lower()
        tech_markers = [
            "—Ç–µ—Ö–Ω", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç", "–ø–∞—Ä–∞–º–µ—Ç—Ä", "–¥–∏–∞–ø–∞–∑–æ–Ω", "–∫–ª–∞—Å—Å —Ç–æ—á–Ω–æ—Å—Ç–∏", "–Ω–∞–≥—Ä—É–∑", "–¥–∞—Ç—á–∏–∫",
            "—Ç–µ–Ω–∑–æ", "—Ä–∞–∑—Ä—ã–≤–Ω", "–º–∞—à–∏–Ω", "—É—Å–∏–ª–∏–µ", "–∫–Ω", "–º–ø–∞", "–Ω—å—é—Ç–æ–Ω", "–º–º", "–≥–æ—Å—Ç", "iso",
            "—Å–µ—Ä—Ç–∏—Ñ", "–º–æ–¥—É–ª—å", "—á–∞—Å—Ç–æ—Ç–∞", "–≤–∏–±—Ä–æ", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª", "–ø—Ä–æ—Ç–æ–∫–æ–ª", "datasheet", "spec"
        ]
        general_markers = ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–∫–æ–Ω—Ç–∞–∫—Ç", "–≥–∞—Ä–∞–Ω—Ç", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–∞–¥—Ä–µ—Å"]
        if any(m in t for m in tech_markers) and not any(m in t for m in general_markers):
            return "tech"
        return "general"

# –£–î–ê–õ–ï–ù–ê –ú–ï–î–õ–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø _max_relevance() - –æ–Ω–∞ —Ç—Ä–∞—Ç–∏–ª–∞ 6.4 —Å–µ–∫—É–Ω–¥—ã!
    # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ –≤ –±—ã—Å—Ç—Ä–æ–π Voximplant –≤–µ—Ä—Å–∏–∏

    def reload(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ RAG-—Ü–µ–ø–æ—á–∫—É."""
        logger.info("üîÉ –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É –∞–≥–µ–Ω—Ç–∞...")
        try:
            self.prompts = self.load_prompts()
            # –û–±–Ω–æ–≤–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π
            self.llm = self._create_llm_from_env(primary=True)
            self._initialize_rag_chain()
            logger.info("‚úÖ –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω —Å –Ω–æ–≤–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –∞–≥–µ–Ω—Ç–∞: {e}", exc_info=True)
            return False

    def get_session_history(self, session_id: str):
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    def get_response_generator(self, user_question: str, session_id: str):
        import time
        start_time = time.time()
        logger.info(f"üïê –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ù–∞—á–∞–ª–æ get_response_generator –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{user_question[:50]}...'")
        
        # –£–ü–†–û–©–ï–ù–ù–ê–Ø –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è (–∫–∞–∫ –≤ Voximplant –≤–µ—Ä—Å–∏–∏)
        route_start = time.time()
        target = self._route_kb(user_question)
        route_time = time.time() - route_start
        logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞ {route_time:.3f}—Å")
        
        # –£–ë–ò–†–ê–ï–ú –ú–ï–î–õ–ï–ù–ù–£–Æ –û–¶–ï–ù–ö–£ –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò!
        # –ë—ã–ª–∞: max_score = self._max_relevance() - 6.4 —Å–µ–∫—É–Ω–¥—ã!
        # –¢–µ–ø–µ—Ä—å: –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
        
        self.last_kb = target
        logger.info(f"–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –≤ –ë–ó: {target}")
        
        setup_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –û–±—â–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–Ω—è–ª–∞ {setup_time:.3f}—Å")
        def _stream_with_chain(use_fallback: bool):
            import time
            stream_start = time.time()
            logger.info(f"üîÑ –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ù–∞—á–∏–Ω–∞–µ–º streaming —Å {'FALLBACK' if use_fallback else 'PRIMARY'} –º–æ–¥–µ–ª—å—é")
            
            chain_local = (
                self.conversational_rag_chain_tech if target == "tech" else self.conversational_rag_chain_general
            )
            
            chain_setup_time = time.time() - stream_start
            logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ø–æ—á–∫–∏ –∑–∞–Ω—è–ª–∞ {chain_setup_time:.3f}—Å")
            
            stream_call_start = time.time()
            stream_local = chain_local.stream(
                {"input": user_question},
                config={"configurable": {"session_id": session_id}},
            )
            stream_call_time = time.time() - stream_call_start
            logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –í—ã–∑–æ–≤ stream() –∑–∞–Ω—è–ª {stream_call_time:.3f}—Å")
            
            first_chunk = True
            chunk_count = 0
            for chunk in stream_local:
                if first_chunk:
                    first_chunk_time = time.time() - stream_start
                    logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {first_chunk_time:.3f}—Å")
                    first_chunk = False
                
                if 'answer' in chunk:
                    chunk_count += 1
                    yield chunk['answer']
            
            total_stream_time = time.time() - stream_start
            logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –í–µ—Å—å streaming –∑–∞–Ω—è–ª {total_stream_time:.3f}—Å, —á–∞–Ω–∫–æ–≤: {chunk_count}")

        def _invoke_non_streaming_with_llm(model_name: str):
            """–õ–æ–∫–∞–ª—å–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–µ —Ü–µ–ø–æ—á–∫–∏ –ø–æ–¥ —É–∫–∞–∑–∞–Ω–Ω—ã–π model_name –∏ –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π."""
            try:
                temperature = float(os.getenv("LLM_TEMPERATURE", "0.2"))
            except ValueError:
                temperature = 0.2
            llm_local = ChatOpenAI(model_name=model_name, temperature=temperature, streaming=False)

            contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [("system", self.prompts["contextualize_q_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
            )
            history_aware_retriever_general = create_history_aware_retriever(llm_local, self.retriever_general, contextualize_q_prompt)
            history_aware_retriever_tech = create_history_aware_retriever(llm_local, self.retriever_tech, contextualize_q_prompt)

            qa_prompt = ChatPromptTemplate.from_messages(
                [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
            )
            question_answer_chain = create_stuff_documents_chain(llm_local, qa_prompt)

            rag_chain_local = create_retrieval_chain(
                history_aware_retriever_tech if target == "tech" else history_aware_retriever_general,
                question_answer_chain,
            )
            runnable = RunnableWithMessageHistory(
                rag_chain_local,
                self.get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer",
            )
            result = runnable.invoke(
                {"input": user_question},
                config={"configurable": {"session_id": session_id}},
            )
            text = result.get("answer", "")
            if text:
                yield text

        # –ü–æ–ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω; –ø—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî –æ–¥–∏–Ω —Ä–∞–∑ —Ñ–æ–ª–±—ç–∫ –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
        try:
            yield from _stream_with_chain(use_fallback=False)
        except Exception as e:
            fb_model = os.getenv("LLM_MODEL_FALLBACK")
            primary_model = os.getenv("LLM_MODEL_PRIMARY", "gpt-4o-mini")
            err_text = str(e).lower()
            # –ï—Å–ª–∏ —Å—Ç—Ä–∏–º –∑–∞–ø—Ä–µ—â—ë–Ω –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π/–º–æ–¥–µ–ª—å—é ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
            if "unsupported_value" in err_text or "verify organization" in err_text or "param': 'stream" in err_text:
                logger.warning("–°—Ç—Ä–∏–º–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏/–º–æ–¥–µ–ª–∏ ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º (PRIMARY)")
                try:
                    yield from _invoke_non_streaming_with_llm(primary_model)
                    return
                except Exception as e_ns:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–µ—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º –Ω–∞ PRIMARY: {e_ns}", exc_info=True)
            # –ï—Å–ª–∏ —Ñ–æ–ª–±—ç–∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –µ–≥–æ (—Å—Ç—Ä–∏–º), –∑–∞—Ç–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ
            if not fb_model or fb_model == primary_model:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–±–µ–∑ —Ñ–æ–ª–±—ç–∫–∞): {e}", exc_info=True)
                return
            logger.warning(f"–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —É–ø–∞–ª–∞: {e}. –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ FALLBACK '{fb_model}'‚Ä¶")
            # –ü–æ—Å—Ç—Ä–æ–∏–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è fallback –æ–¥–∏–Ω —Ä–∞–∑
            try:
                if not self._fallback_chains_built:
                    self.fallback_llm = self._create_llm_from_env(primary=False)
                    self._build_chains_for_llm(self.fallback_llm)
                    self._fallback_chains_built = True
                try:
                    yield from _stream_with_chain(use_fallback=True)
                    return
                except Exception as e_fb_stream:
                    logger.warning(f"–§–æ–ª–±—ç–∫ –≤ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –Ω–µ —É–¥–∞–ª—Å—è: {e_fb_stream}. –ü—Ä–æ–±—É–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ‚Ä¶")
                    yield from _invoke_non_streaming_with_llm(fb_model)
            except Exception as e2:
                logger.error(f"–§–æ–ª–±—ç–∫ –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –Ω–µ —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å: {e2}", exc_info=True)
                return
