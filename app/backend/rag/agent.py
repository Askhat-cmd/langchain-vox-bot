from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging
import json
import os

load_dotenv()

logger = logging.getLogger(__name__)

class Agent:
    def __init__(self) -> None:
        logger.info("--- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ê–≥–µ–Ω—Ç–∞ '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' ---")
        # –¢–µ–∫—É—â–∞—è LLM –∏ –ª–µ–Ω–∏–≤—ã–π fallback
        self.llm = self._create_llm_from_env(primary=True)
        self.fallback_llm = None
        self._fallback_chains_built = False
        self.store = {}
        self.last_kb = "general"
        try:
            self.prompts = self.load_prompts()
        except (ValueError, FileNotFoundError, json.JSONDecodeError) as e:
            logger.critical(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤. {e}", exc_info=True)
            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∑–∞–ø—É—Å–∫ —Å –Ω–µ–≤–µ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            raise SystemExit(f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {e}")

        self._initialize_rag_chain()
        logger.info("--- –ê–≥–µ–Ω—Ç '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ---")

    def _create_llm_from_env(self, primary: bool) -> ChatOpenAI:
        """–°–æ–∑–¥–∞—ë—Ç LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.
        primary=True ‚Üí LLM_MODEL_PRIMARY, –∏–Ω–∞—á–µ LLM_MODEL_FALLBACK.
        """
        model_env_key = "LLM_MODEL_PRIMARY" if primary else "LLM_MODEL_FALLBACK"
        model_name = os.getenv(model_env_key, os.getenv("LLM_MODEL_PRIMARY", "gpt-4o-mini"))
        try:
            temperature = float(os.getenv("LLM_TEMPERATURE", "0.2"))
        except ValueError:
            temperature = 0.2
        logger.info(f"LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'PRIMARY' if primary else 'FALLBACK'} model='{model_name}', temperature={temperature}")
        return ChatOpenAI(model_name=model_name, temperature=temperature, streaming=True)

    def load_prompts(self):
        prompts_file = os.getenv("PROMPTS_FILE_PATH")
        if not prompts_file:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PROMPTS_FILE_PATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: '{prompts_file}'")
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {prompts_file}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ {prompts_file}: {e}")
            raise

    def _initialize_rag_chain(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç/–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ RAG-—Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π LLM."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ RAG-—Ü–µ–ø–æ—á–∫–∏...")

        persist_directory = os.getenv("PERSIST_DIRECTORY")
        if not persist_directory:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PERSIST_DIRECTORY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –≤ '{persist_directory}'...")
        embeddings = OpenAIEmbeddings(chunk_size=1000)
        self.db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        kb_k = int(os.getenv("KB_TOP_K", "3"))
        self.retriever_general = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "general"}}
        )
        self.retriever_tech = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "tech"}}
        )
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ.")

        # –°—Ç—Ä–æ–∏–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π LLM
        self._build_chains_for_llm(self.llm)
        # –°–±—Ä–æ—Å–∏–º –∫—ç—à —Ñ–æ–ª–±—ç–∫–∞
        self._fallback_chains_built = False
        self.fallback_llm = None
        logger.info("--- RAG-—Ü–µ–ø–æ—á–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞ ---")

    def _build_chains_for_llm(self, llm: ChatOpenAI):
        """–°—Ç—Ä–æ–∏—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π LLM."""
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["contextualize_q_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        history_aware_retriever_general = create_history_aware_retriever(llm, self.retriever_general, contextualize_q_prompt)
        history_aware_retriever_tech = create_history_aware_retriever(llm, self.retriever_tech, contextualize_q_prompt)

        qa_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

        rag_chain_general = create_retrieval_chain(history_aware_retriever_general, question_answer_chain)
        rag_chain_tech = create_retrieval_chain(history_aware_retriever_tech, question_answer_chain)

        self.conversational_rag_chain_general = RunnableWithMessageHistory(
            rag_chain_general,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        self.conversational_rag_chain_tech = RunnableWithMessageHistory(
            rag_chain_tech,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

    def _route_kb(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: general | tech.
        –í—ã–±–∏—Ä–∞–µ–º 'tech', –µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã; –∏–Ω–∞—á–µ 'general'."""
        if not text:
            return "general"
        t = text.lower()
        tech_markers = [
            "—Ç–µ—Ö–Ω", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç", "–ø–∞—Ä–∞–º–µ—Ç—Ä", "–¥–∏–∞–ø–∞–∑–æ–Ω", "–∫–ª–∞—Å—Å —Ç–æ—á–Ω–æ—Å—Ç–∏", "–Ω–∞–≥—Ä—É–∑", "–¥–∞—Ç—á–∏–∫",
            "—Ç–µ–Ω–∑–æ", "—Ä–∞–∑—Ä—ã–≤–Ω", "–º–∞—à–∏–Ω", "—É—Å–∏–ª–∏–µ", "–∫–Ω", "–º–ø–∞", "–Ω—å—é—Ç–æ–Ω", "–º–º", "–≥–æ—Å—Ç", "iso",
            "—Å–µ—Ä—Ç–∏—Ñ", "–º–æ–¥—É–ª—å", "—á–∞—Å—Ç–æ—Ç–∞", "–≤–∏–±—Ä–æ", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª", "–ø—Ä–æ—Ç–æ–∫–æ–ª", "datasheet", "spec"
        ]
        general_markers = ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–∫–æ–Ω—Ç–∞–∫—Ç", "–≥–∞—Ä–∞–Ω—Ç", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–∞–¥—Ä–µ—Å"]
        if any(m in t for m in tech_markers) and not any(m in t for m in general_markers):
            return "tech"
        return "general"

    def _max_relevance(self, question: str, kb: str, k: int) -> float:
        try:
            # –î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ—Ä–æ–≥–∞ –±–µ—Ä—ë–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ç–æ—Ä–∞
            res = self.db.similarity_search_with_relevance_scores(
                question, k=k, filter={"kb": kb}
            )
            if not res:
                return 0.0
            return max(score for _, score in res)
        except Exception:
            return 1.0  # –µ—Å–ª–∏ —Å—Ç–æ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ü–µ–Ω–∫—É, –Ω–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏–º —Ñ–æ–ª–±—ç–∫

    def reload(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ RAG-—Ü–µ–ø–æ—á–∫—É."""
        logger.info("üîÉ –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É –∞–≥–µ–Ω—Ç–∞...")
        try:
            self.prompts = self.load_prompts()
            # –û–±–Ω–æ–≤–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π
            self.llm = self._create_llm_from_env(primary=True)
            self._initialize_rag_chain()
            logger.info("‚úÖ –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω —Å –Ω–æ–≤–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –∞–≥–µ–Ω—Ç–∞: {e}", exc_info=True)
            return False

    def get_session_history(self, session_id: str):
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    def get_response_generator(self, user_question: str, session_id: str):
        target = self._route_kb(user_question)
        # –ü–æ—Ä–æ–≥ –∏ k –±–µ—Ä—ë–º –∏–∑ env (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2 –∏ 3)
        threshold = float(os.getenv("KB_FALLBACK_THRESHOLD", "0.2"))
        k = int(os.getenv("KB_TOP_K", "3"))
        # –û—Ü–µ–Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ë–ó; –ø—Ä–∏ –Ω–∏–∑–∫–æ–π ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –≤—Ç–æ—Ä—É—é
        max_score = self._max_relevance(user_question, target, k)
        alt = "tech" if target == "general" else "general"
        if max_score < threshold:
            alt_score = self._max_relevance(user_question, alt, k)
            if alt_score > max_score:
                logger.info(
                    f"–ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å ({max_score:.2f}) –¥–ª—è {target} ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ {alt} ({alt_score:.2f})"
                )
                target = alt
        self.last_kb = target
        logger.info(f"–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –≤ –ë–ó: {target}")
        def _stream_with_chain(use_fallback: bool):
            chain_local = (
                self.conversational_rag_chain_tech if target == "tech" else self.conversational_rag_chain_general
            )
            stream_local = chain_local.stream(
                {"input": user_question},
                config={"configurable": {"session_id": session_id}},
            )
            for chunk in stream_local:
                if 'answer' in chunk:
                    yield chunk['answer']

        def _invoke_non_streaming_with_llm(model_name: str):
            """–õ–æ–∫–∞–ª—å–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–µ —Ü–µ–ø–æ—á–∫–∏ –ø–æ–¥ —É–∫–∞–∑–∞–Ω–Ω—ã–π model_name –∏ –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π."""
            try:
                temperature = float(os.getenv("LLM_TEMPERATURE", "0.2"))
            except ValueError:
                temperature = 0.2
            llm_local = ChatOpenAI(model_name=model_name, temperature=temperature, streaming=False)

            contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [("system", self.prompts["contextualize_q_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
            )
            history_aware_retriever_general = create_history_aware_retriever(llm_local, self.retriever_general, contextualize_q_prompt)
            history_aware_retriever_tech = create_history_aware_retriever(llm_local, self.retriever_tech, contextualize_q_prompt)

            qa_prompt = ChatPromptTemplate.from_messages(
                [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
            )
            question_answer_chain = create_stuff_documents_chain(llm_local, qa_prompt)

            rag_chain_local = create_retrieval_chain(
                history_aware_retriever_tech if target == "tech" else history_aware_retriever_general,
                question_answer_chain,
            )
            runnable = RunnableWithMessageHistory(
                rag_chain_local,
                self.get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer",
            )
            result = runnable.invoke(
                {"input": user_question},
                config={"configurable": {"session_id": session_id}},
            )
            text = result.get("answer", "")
            if text:
                yield text

        # –ü–æ–ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω; –ø—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî –æ–¥–∏–Ω —Ä–∞–∑ —Ñ–æ–ª–±—ç–∫ –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
        try:
            yield from _stream_with_chain(use_fallback=False)
        except Exception as e:
            fb_model = os.getenv("LLM_MODEL_FALLBACK")
            primary_model = os.getenv("LLM_MODEL_PRIMARY", "gpt-4o-mini")
            err_text = str(e).lower()
            # –ï—Å–ª–∏ —Å—Ç—Ä–∏–º –∑–∞–ø—Ä–µ—â—ë–Ω –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π/–º–æ–¥–µ–ª—å—é ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
            if "unsupported_value" in err_text or "verify organization" in err_text or "param': 'stream" in err_text:
                logger.warning("–°—Ç—Ä–∏–º–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏/–º–æ–¥–µ–ª–∏ ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º (PRIMARY)")
                try:
                    yield from _invoke_non_streaming_with_llm(primary_model)
                    return
                except Exception as e_ns:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–µ—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º –Ω–∞ PRIMARY: {e_ns}", exc_info=True)
            # –ï—Å–ª–∏ —Ñ–æ–ª–±—ç–∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –µ–≥–æ (—Å—Ç—Ä–∏–º), –∑–∞—Ç–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ
            if not fb_model or fb_model == primary_model:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–±–µ–∑ —Ñ–æ–ª–±—ç–∫–∞): {e}", exc_info=True)
                return
            logger.warning(f"–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —É–ø–∞–ª–∞: {e}. –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ FALLBACK '{fb_model}'‚Ä¶")
            # –ü–æ—Å—Ç—Ä–æ–∏–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è fallback –æ–¥–∏–Ω —Ä–∞–∑
            try:
                if not self._fallback_chains_built:
                    self.fallback_llm = self._create_llm_from_env(primary=False)
                    self._build_chains_for_llm(self.fallback_llm)
                    self._fallback_chains_built = True
                try:
                    yield from _stream_with_chain(use_fallback=True)
                    return
                except Exception as e_fb_stream:
                    logger.warning(f"–§–æ–ª–±—ç–∫ –≤ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –Ω–µ —É–¥–∞–ª—Å—è: {e_fb_stream}. –ü—Ä–æ–±—É–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ‚Ä¶")
                    yield from _invoke_non_streaming_with_llm(fb_model)
            except Exception as e2:
                logger.error(f"–§–æ–ª–±—ç–∫ –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –Ω–µ —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å: {e2}", exc_info=True)
                return
