from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from dotenv import load_dotenv
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging
import json
import os
import redis
import hashlib
import time
from typing import List

class CachedOpenAIEmbeddings(OpenAIEmbeddings):
    """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ OpenAI Embeddings –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, redis_client, **kwargs):
        super().__init__(**kwargs)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–∫—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à-–¥–∞–Ω–Ω—ã—Ö
        self._cache_data = {
            'redis_client': redis_client,
            'cache_prefix': "embedding_cache:",
            'cache_ttl': 3600 * 24 * 7  # 7 –¥–Ω–µ–π
        }
        
    def _get_cache_key(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫–µ—à–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        return f"{self._cache_data['cache_prefix']}{text_hash}"
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """–ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        results = []
        texts_to_embed = []
        text_indices = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        for i, text in enumerate(texts):
            cache_key = self._get_cache_key(text)
            cached_embedding = self._cache_data['redis_client'].get(cache_key)
            
            if cached_embedding:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º embedding –∏–∑ –∫–µ—à–∞
                embedding = json.loads(cached_embedding)
                results.append(embedding)
                logger.debug(f"‚úÖ Embedding –∏–∑ –∫–µ—à–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {i}")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embedding
                texts_to_embed.append(text)
                text_indices.append(i)
                results.append(None)  # –ó–∞–≥–ª—É—à–∫–∞
        
        # –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–µ—à–µ
        if texts_to_embed:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–µ–º {len(texts_to_embed)} –Ω–æ–≤—ã—Ö embeddings")
            new_embeddings = super().embed_documents(texts_to_embed)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for i, (text, embedding) in enumerate(zip(texts_to_embed, new_embeddings)):
                cache_key = self._get_cache_key(text)
                self._cache_data['redis_client'].setex(cache_key, self._cache_data['cache_ttl'], json.dumps(embedding))
                
                # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                original_index = text_indices[i]
                results[original_index] = embedding
                logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–µ—à embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {original_index}")
        
        return results
    
    def embed_query(self, text: str) -> List[float]:
        """
        üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ query embeddings
        –¶–µ–ª—å: –ò–∑–±–µ–∂–∞—Ç—å OpenAI API –≤—ã–∑–æ–≤–æ–≤ –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—Ä–æ—Å–µ (—ç–∫–æ–Ω–æ–º–∏—è 0.7-0.9—Å)
        """
        cache_key = self._get_cache_key(text)
        cached_embedding = self._cache_data['redis_client'].get(cache_key)
        
        if cached_embedding:
            logger.info(f"‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Embedding –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –∫–µ—à–∞ (—ç–∫–æ–Ω–æ–º–∏—è ~0.8—Å)")
            return json.loads(cached_embedding)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π embedding –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –∫–µ—à–µ
        logger.warning(f"üîÑ –ú–ï–î–õ–ï–ù–ù–û: –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (OpenAI API ~0.8—Å)")
        start_time = time.time()
        embedding = super().embed_query(text)
        elapsed = time.time() - start_time
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º TTL
        self._cache_data['redis_client'].setex(cache_key, self._cache_data['cache_ttl'], json.dumps(embedding))
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–µ—à embedding –∑–∞–ø—Ä–æ—Å–∞ (–∑–∞–Ω—è–ª–æ {elapsed:.3f}—Å)")
        
        return embedding

load_dotenv()

logger = logging.getLogger(__name__)

class Agent:
    def __init__(self) -> None:
        logger.info("--- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ê–≥–µ–Ω—Ç–∞ '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' ---")
        # –¢–µ–∫—É—â–∞—è LLM –∏ –ª–µ–Ω–∏–≤—ã–π fallback
        self.llm = self._create_llm_from_env(primary=True)
        self.fallback_llm = None
        self._fallback_chains_built = False
        self.store = {}
        self.last_kb = "general"
        
        # –ù–û–í–û–ï: Redis –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è embeddings
        try:
            self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
            self.redis_client.ping()  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            self.cache_enabled = True
            logger.info("‚úÖ Redis –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ")
        except Exception as e:
            self.redis_client = None
            self.cache_enabled = False
            logger.warning(f"‚ö†Ô∏è Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ: {e}")
        try:
            self.prompts = self.load_prompts()
        except (ValueError, FileNotFoundError, json.JSONDecodeError) as e:
            logger.critical(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤. {e}", exc_info=True)
            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∑–∞–ø—É—Å–∫ —Å –Ω–µ–≤–µ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            raise SystemExit(f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {e}")

        self._initialize_rag_chain()
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: Pre-warm –∫–µ—à –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        self._prewarm_embedding_cache()
        
        logger.info("--- –ê–≥–µ–Ω—Ç '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ---")

    def _prewarm_embedding_cache(self):
        """
        üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ embeddings –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∞–≥–µ–Ω—Ç–∞, —á—Ç–æ–±—ã –ø–µ—Ä–≤—ã–µ –∑–≤–æ–Ω–∫–∏ –±—ã–ª–∏ –±—ã—Å—Ç—Ä—ã–º–∏
        """
        if not self.cache_enabled:
            logger.info("‚ö†Ô∏è –ö–µ—à –æ—Ç–∫–ª—é—á–µ–Ω, pre-warming –ø—Ä–æ–ø—É—â–µ–Ω")
            return
        
        # –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è pre-warming
        common_questions = [
            "—Ç–≤–µ—Ä–¥–æ–º–µ—Ä",
            "—Ä–∞–∑—Ä—ã–≤–Ω–∞—è –º–∞—à–∏–Ω–∞",
            "–∏—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–µ—Å—Å",
            "—Ü–µ–Ω–∞",
            "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏",
            "–¥–æ—Å—Ç–∞–≤–∫–∞",
            "–£ –≤–∞—Å –µ—Å—Ç—å –ø—Ä–µ—Å—Å –∏—Å–ø—ã—Ç–∞—Ç–µ–ª—å–Ω—ã–π",
            "–ö–∞–∫–∏–µ –µ—Å—Ç—å —Ç–≤–µ—Ä–¥–æ–º–µ—Ä—ã",
            "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç —Ä–∞–∑—Ä—ã–≤–Ω–∞—è –º–∞—à–∏–Ω–∞",
            "–ö–∞–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —É –†–≠–ú",
            "–ï—Å—Ç—å –ª–∏ –≥–∞—Ä–∞–Ω—Ç–∏—è",
            "–ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –∑–∞–∫–∞–∑"
        ]
        
        logger.info(f"üî• Pre-warming –∫–µ—à–∞ –¥–ª—è {len(common_questions)} –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º embeddings –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è pre-warming
        if isinstance(self.db._embedding_function, CachedOpenAIEmbeddings):
            warmed = 0
            for question in common_questions:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –≤ –∫–µ—à–µ
                    cache_key = self.db._embedding_function._get_cache_key(question)
                    if not self.redis_client.get(cache_key):
                        # –°–æ–∑–¥–∞–µ–º embedding (–æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –∫–µ—à)
                        self.db._embedding_function.embed_query(question)
                        warmed += 1
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ pre-warming –¥–ª—è '{question}': {e}")
            
            logger.info(f"‚úÖ Pre-warming –∑–∞–≤–µ—Ä—à–µ–Ω: {warmed} –Ω–æ–≤—ã—Ö, {len(common_questions) - warmed} —É–∂–µ –≤ –∫–µ—à–µ")
        else:
            logger.warning("‚ö†Ô∏è Embeddings –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ, pre-warming –ø—Ä–æ–ø—É—â–µ–Ω")
    
    def _get_cache_key(self, text: str, kb: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫–µ—à–∞ –¥–ª—è embedding –∑–∞–ø—Ä–æ—Å–∞."""
        combined = f"{text}:{kb}:{self.last_kb}"
        return f"emb:{hashlib.md5(combined.encode()).hexdigest()}"
    
    def _get_cached_documents(self, text: str, kb: str):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Redis."""
        if not self.cache_enabled:
            return None
        
        cache_key = self._get_cache_key(text, kb)
        try:
            cached = self.redis_client.get(cache_key)
            if cached:
                logger.info(f"üéØ –ö–ï–®–ò–†–û–í–ê–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è: {text[:30]}...")
                return json.loads(cached)
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
        return None
    
    def _cache_documents(self, text: str, kb: str, documents):
        """–ö–µ—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ Redis."""
        if not self.cache_enabled or not documents:
            return
            
        cache_key = self._get_cache_key(text, kb)
        try:
            # –ö–µ—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
            doc_contents = [{"content": doc.page_content, "metadata": doc.metadata} for doc in documents]
            self.redis_client.setex(
                cache_key, 
                3600,  # 1 —á–∞—Å TTL
                json.dumps(doc_contents)
            )
            logger.debug(f"üì¶ –ö–ï–®–ò–†–û–í–ê–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–µ—à")
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫–µ—à: {e}")

    def _create_llm_from_env(self, primary: bool) -> ChatOpenAI:
        """–°–æ–∑–¥–∞—ë—Ç LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.
        primary=True ‚Üí LLM_MODEL_PRIMARY, –∏–Ω–∞—á–µ LLM_MODEL_FALLBACK.
        """
        model_env_key = "LLM_MODEL_PRIMARY" if primary else "LLM_MODEL_FALLBACK"
        model_name = os.getenv(model_env_key, os.getenv("LLM_MODEL_PRIMARY", "gpt-4o-mini"))
        try:
            temperature = float(os.getenv("LLM_TEMPERATURE", "0.2"))
        except ValueError:
            temperature = 0.2
        logger.info(f"LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'PRIMARY' if primary else 'FALLBACK'} model='{model_name}', temperature={temperature}")
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ –∏ —Ç–∞–π–º–∞—É—Ç
        try:
            max_tokens = int(os.getenv("LLM_MAX_TOKENS", "128"))
        except ValueError:
            max_tokens = 128

        return ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            streaming=True,
            request_timeout=12,  # –∫–æ—Ä–æ—á–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–∞–∑–∞
            max_retries=1,
            max_tokens=max_tokens
        )

    def load_prompts(self):
        prompts_file = os.getenv("PROMPTS_FILE_PATH")
        if not prompts_file:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PROMPTS_FILE_PATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: '{prompts_file}'")
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {prompts_file}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ {prompts_file}: {e}")
            raise

    def _initialize_rag_chain(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç/–ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ RAG-—Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π LLM."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ RAG-—Ü–µ–ø–æ—á–∫–∏...")

        persist_directory = os.getenv("PERSIST_DIRECTORY")
        if not persist_directory:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PERSIST_DIRECTORY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –≤ '{persist_directory}'...")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∑–¥–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings
        if self.cache_enabled:
            embeddings = CachedOpenAIEmbeddings(
                redis_client=self.redis_client,
                chunk_size=1000
            )
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings")
        else:
            embeddings = OpenAIEmbeddings(chunk_size=1000)
            logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ embeddings (–∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ)")
            
        self.db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: —É–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        kb_k = int(os.getenv("KB_TOP_K", "1"))
        self.retriever_general = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "general"}}
        )
        self.retriever_tech = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "tech"}}
        )
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ.")

        # –°—Ç—Ä–æ–∏–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π LLM
        self._build_chains_for_llm(self.llm)
        # –°–±—Ä–æ—Å–∏–º –∫—ç—à —Ñ–æ–ª–±—ç–∫–∞
        self._fallback_chains_built = False
        self.fallback_llm = None
        logger.info("--- RAG-—Ü–µ–ø–æ—á–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞ ---")

    def _build_chains_for_llm(self, llm: ChatOpenAI):
        """–°—Ç—Ä–æ–∏—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π LLM."""
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–±–∏—Ä–∞–µ–º history_aware_retriever –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        # –í–º–µ—Å—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ LLM –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ retriever'—ã
        logger.info("üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ retriever'—ã –±–µ–∑ history_aware –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ retriever'—ã –≤–º–µ—Å—Ç–æ history_aware
        rag_chain_general = create_retrieval_chain(self.retriever_general, question_answer_chain)
        rag_chain_tech = create_retrieval_chain(self.retriever_tech, question_answer_chain)

        self.conversational_rag_chain_general = RunnableWithMessageHistory(
            rag_chain_general,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        self.conversational_rag_chain_tech = RunnableWithMessageHistory(
            rag_chain_tech,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )

    def _route_kb(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: general | tech.
        –í—ã–±–∏—Ä–∞–µ–º 'tech', –µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã; –∏–Ω–∞—á–µ 'general'."""
        if not text:
            return "general"
        t = text.lower()
        tech_markers = [
            "—Ç–µ—Ö–Ω", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç", "–ø–∞—Ä–∞–º–µ—Ç—Ä", "–¥–∏–∞–ø–∞–∑–æ–Ω", "–∫–ª–∞—Å—Å —Ç–æ—á–Ω–æ—Å—Ç–∏", "–Ω–∞–≥—Ä—É–∑", "–¥–∞—Ç—á–∏–∫",
            "—Ç–µ–Ω–∑–æ", "—Ä–∞–∑—Ä—ã–≤–Ω", "–º–∞—à–∏–Ω", "—É—Å–∏–ª–∏–µ", "–∫–Ω", "–º–ø–∞", "–Ω—å—é—Ç–æ–Ω", "–º–º", "–≥–æ—Å—Ç", "iso",
            "—Å–µ—Ä—Ç–∏—Ñ", "–º–æ–¥—É–ª—å", "—á–∞—Å—Ç–æ—Ç–∞", "–≤–∏–±—Ä–æ", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª", "–ø—Ä–æ—Ç–æ–∫–æ–ª", "datasheet", "spec"
        ]
        general_markers = ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–∫–æ–Ω—Ç–∞–∫—Ç", "–≥–∞—Ä–∞–Ω—Ç", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–∞–¥—Ä–µ—Å"]
        if any(m in t for m in tech_markers) and not any(m in t for m in general_markers):
            return "tech"
        return "general"

# –£–î–ê–õ–ï–ù–ê –ú–ï–î–õ–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø _max_relevance() - –æ–Ω–∞ —Ç—Ä–∞—Ç–∏–ª–∞ 6.4 —Å–µ–∫—É–Ω–¥—ã!
    # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª–∞ –≤ –±—ã—Å—Ç—Ä–æ–π Voximplant –≤–µ—Ä—Å–∏–∏

    def reload(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ RAG-—Ü–µ–ø–æ—á–∫—É."""
        logger.info("üîÉ –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É –∞–≥–µ–Ω—Ç–∞...")
        try:
            self.prompts = self.load_prompts()
            # –û–±–Ω–æ–≤–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π
            self.llm = self._create_llm_from_env(primary=True)
            self._initialize_rag_chain()
            logger.info("‚úÖ –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω —Å –Ω–æ–≤–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –∞–≥–µ–Ω—Ç–∞: {e}", exc_info=True)
            return False

    def get_session_history(self, session_id: str):
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    def get_response_generator(self, user_question: str, session_id: str):
        import time
        start_time = time.time()
        logger.info(f"üïê –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ù–∞—á–∞–ª–æ get_response_generator –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: '{user_question[:50]}...'")
        
        # –£–ü–†–û–©–ï–ù–ù–ê–Ø –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è (–∫–∞–∫ –≤ Voximplant –≤–µ—Ä—Å–∏–∏)
        route_start = time.time()
        target = self._route_kb(user_question)
        route_time = time.time() - route_start
        logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞ {route_time:.3f}—Å")
        
        # –£–ë–ò–†–ê–ï–ú –ú–ï–î–õ–ï–ù–ù–£–Æ –û–¶–ï–ù–ö–£ –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò!
        # –ë—ã–ª–∞: max_score = self._max_relevance() - 6.4 —Å–µ–∫—É–Ω–¥—ã!
        # –¢–µ–ø–µ—Ä—å: –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
        
        self.last_kb = target
        logger.info(f"–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –≤ –ë–ó: {target}")
        
        setup_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –û–±—â–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–Ω—è–ª–∞ {setup_time:.3f}—Å")
        def _stream_with_chain(use_fallback: bool):
            import time
            stream_start = time.time()
            logger.info(f"üîÑ –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ù–∞—á–∏–Ω–∞–µ–º streaming —Å {'FALLBACK' if use_fallback else 'PRIMARY'} –º–æ–¥–µ–ª—å—é")
            
            chain_local = (
                self.conversational_rag_chain_tech if target == "tech" else self.conversational_rag_chain_general
            )
            
            chain_setup_time = time.time() - stream_start
            logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ü–µ–ø–æ—á–∫–∏ –∑–∞–Ω—è–ª–∞ {chain_setup_time:.3f}—Å")
            
            stream_call_start = time.time()
            stream_local = chain_local.stream(
                {"input": user_question},
                config={"configurable": {"session_id": session_id}},
            )
            stream_call_time = time.time() - stream_call_start
            logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –í—ã–∑–æ–≤ stream() –∑–∞–Ω—è–ª {stream_call_time:.3f}—Å")
            
            first_chunk = True
            chunk_count = 0
            for chunk in stream_local:
                if first_chunk:
                    first_chunk_time = time.time() - stream_start
                    logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {first_chunk_time:.3f}—Å")
                    first_chunk = False
                
                if 'answer' in chunk:
                    chunk_count += 1
                    yield chunk['answer']
            
            total_stream_time = time.time() - stream_start
            logger.info(f"‚è±Ô∏è –ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï: –í–µ—Å—å streaming –∑–∞–Ω—è–ª {total_stream_time:.3f}—Å, —á–∞–Ω–∫–æ–≤: {chunk_count}")

        def _invoke_non_streaming_with_llm(model_name: str):
            """–õ–æ–∫–∞–ª—å–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–µ —Ü–µ–ø–æ—á–∫–∏ –ø–æ–¥ —É–∫–∞–∑–∞–Ω–Ω—ã–π model_name –∏ –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π."""
            try:
                temperature = float(os.getenv("LLM_TEMPERATURE", "0.2"))
            except ValueError:
                temperature = 0.2
            llm_local = ChatOpenAI(model_name=model_name, temperature=temperature, streaming=False)

            contextualize_q_prompt = ChatPromptTemplate.from_messages(
                [("system", self.prompts["contextualize_q_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
            )
            history_aware_retriever_general = create_history_aware_retriever(llm_local, self.retriever_general, contextualize_q_prompt)
            history_aware_retriever_tech = create_history_aware_retriever(llm_local, self.retriever_tech, contextualize_q_prompt)

            qa_prompt = ChatPromptTemplate.from_messages(
                [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
            )
            question_answer_chain = create_stuff_documents_chain(llm_local, qa_prompt)

            rag_chain_local = create_retrieval_chain(
                history_aware_retriever_tech if target == "tech" else history_aware_retriever_general,
                question_answer_chain,
            )
            runnable = RunnableWithMessageHistory(
                rag_chain_local,
                self.get_session_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer",
            )
            result = runnable.invoke(
                {"input": user_question},
                config={"configurable": {"session_id": session_id}},
            )
            text = result.get("answer", "")
            if text:
                yield text

        # –ü–æ–ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω; –ø—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî –æ–¥–∏–Ω —Ä–∞–∑ —Ñ–æ–ª–±—ç–∫ –Ω–∞ –∑–∞–ø–∞—Å–Ω—É—é –º–æ–¥–µ–ª—å
        try:
            yield from _stream_with_chain(use_fallback=False)
        except Exception as e:
            fb_model = os.getenv("LLM_MODEL_FALLBACK")
            primary_model = os.getenv("LLM_MODEL_PRIMARY", "gpt-4o-mini")
            err_text = str(e).lower()
            # –ï—Å–ª–∏ —Å—Ç—Ä–∏–º –∑–∞–ø—Ä–µ—â—ë–Ω –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π/–º–æ–¥–µ–ª—å—é ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
            if "unsupported_value" in err_text or "verify organization" in err_text or "param': 'stream" in err_text:
                logger.warning("–°—Ç—Ä–∏–º–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Ç–µ–∫—É—â–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏/–º–æ–¥–µ–ª–∏ ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º (PRIMARY)")
                try:
                    yield from _invoke_non_streaming_with_llm(primary_model)
                    return
                except Exception as e_ns:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–µ—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —Ä–µ–∂–∏–º –Ω–∞ PRIMARY: {e_ns}", exc_info=True)
            # –ï—Å–ª–∏ —Ñ–æ–ª–±—ç–∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –µ–≥–æ (—Å—Ç—Ä–∏–º), –∑–∞—Ç–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ
            if not fb_model or fb_model == primary_model:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–±–µ–∑ —Ñ–æ–ª–±—ç–∫–∞): {e}", exc_info=True)
                return
            logger.warning(f"–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å —É–ø–∞–ª–∞: {e}. –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ FALLBACK '{fb_model}'‚Ä¶")
            # –ü–æ—Å—Ç—Ä–æ–∏–º —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è fallback –æ–¥–∏–Ω —Ä–∞–∑
            try:
                if not self._fallback_chains_built:
                    self.fallback_llm = self._create_llm_from_env(primary=False)
                    self._build_chains_for_llm(self.fallback_llm)
                    self._fallback_chains_built = True
                try:
                    yield from _stream_with_chain(use_fallback=True)
                    return
                except Exception as e_fb_stream:
                    logger.warning(f"–§–æ–ª–±—ç–∫ –≤ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –Ω–µ —É–¥–∞–ª—Å—è: {e_fb_stream}. –ü—Ä–æ–±—É–µ–º –ù–ï—Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ‚Ä¶")
                    yield from _invoke_non_streaming_with_llm(fb_model)
            except Exception as e2:
                logger.error(f"–§–æ–ª–±—ç–∫ –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –Ω–µ —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å: {e2}", exc_info=True)
                return

    def _is_sentence_complete(self, text: str, min_length: int = 50, max_length: int = 200) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è chunking.
        –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∞—à–∏ –º–∞—Ä–∫–µ—Ä—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ "|" –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """
        if not text or len(text) < min_length:
            return False
            
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(text) >= max_length:
            return True
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ "|"
        if text.endswith(('.|', '?|', '!|')):
            return True
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if text.endswith(('.', '?', '!', ':', ';')):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            if len(text) >= min_length:
                return True
                
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥—ã –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É (–º–æ–≥—É—Ç –±—ã—Ç—å –≤ markdown)
        if '\n' in text and len(text) >= min_length:
            return True
            
        return False

    def get_chunked_response_generator(self, user_question: str, session_id: str):
        """
        –Ø–î–†–û –°–ò–°–¢–ï–ú–´: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞ —Å —É–º–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ TTS.
        –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º get_response_generator, –Ω–æ –±—É—Ñ–µ—Ä–∏–∑—É–µ—Ç –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        
        –¶–ï–õ–¨: –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ —á–µ—Ä–µ–∑ 0.6-0.8—Å –æ—Ç –Ω–∞—á–∞–ª–∞ AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        """
        import time
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ .env
        chunk_min_length = int(os.getenv("CHUNK_MIN_LENGTH", "50"))
        chunk_max_length = int(os.getenv("CHUNK_MAX_LENGTH", "200"))
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π streaming generator
        response_stream = self.get_response_generator(user_question, session_id)
        
        buffer = ""
        chunk_count = 0
        start_time = time.time()
        
        logger.info(f"ü§ñ –ù–∞—á–∞–ª–æ chunked –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è: '{user_question[:50]}...'")
        
        try:
            for token in response_stream:
                buffer += token
                
                # –£–ú–ù–ê–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                if self._is_sentence_complete(buffer, chunk_min_length, chunk_max_length):
                    sentence = buffer.strip()
                    buffer = ""
                    
                    if sentence:  # –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —á–∞–Ω–∫–∏
                        chunk_count += 1
                        elapsed = time.time() - start_time
                        
                        logger.info(f"‚ö° Chunk {chunk_count} ready in {elapsed:.2f}s: '{sentence[:30]}...'")
                        
                        # –ü–ï–†–í–´–ô –ß–ê–ù–ö - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞
                        if chunk_count == 1:
                            logger.info(f"üéØ FIRST CHUNK TIME: {elapsed:.2f}s (target: <0.8s)")
                        
                        yield {
                            "text": sentence,
                            "chunk_number": chunk_count,
                            "elapsed_time": elapsed,
                            "kb": self.last_kb,
                            "is_first": chunk_count == 1
                        }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞ (—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞)
            if buffer.strip():
                chunk_count += 1
                elapsed = time.time() - start_time
                logger.info(f"üèÅ Final chunk {chunk_count}: '{buffer.strip()[:30]}...'")
                
                yield {
                    "text": buffer.strip(),
                    "chunk_number": chunk_count,
                    "elapsed_time": elapsed,
                    "kb": self.last_kb,
                    "is_final": True
                }
                
        except Exception as e:
            logger.error(f"‚ùå Chunked generator error: {e}", exc_info=True)
            # –ö—Ä–∏—Ç–∏—á–Ω–æ: fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
            logger.warning("üîÑ Falling back to regular generator")
            full_response = ""
            for token in self.get_response_generator(user_question, session_id):
                full_response += token
            yield {
                "text": full_response,
                "chunk_number": 1,
                "elapsed_time": time.time() - start_time,
                "kb": self.last_kb,
                "fallback": True
            }
