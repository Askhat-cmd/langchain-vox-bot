from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging
import json
import os

load_dotenv()

logger = logging.getLogger(__name__)

class Agent:
    def __init__(self) -> None:
        logger.info("--- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ê–≥–µ–Ω—Ç–∞ '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' ---")
        self.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2, streaming=True)
        self.store = {}
        try:
            self.prompts = self.load_prompts()
        except (ValueError, FileNotFoundError, json.JSONDecodeError) as e:
            logger.critical(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤. {e}", exc_info=True)
            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∑–∞–ø—É—Å–∫ —Å –Ω–µ–≤–µ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            raise SystemExit(f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {e}")

        self._initialize_rag_chain()
        logger.info("--- –ê–≥–µ–Ω—Ç '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ---")

    def load_prompts(self):
        prompts_file = os.getenv("PROMPTS_FILE_PATH")
        if not prompts_file:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PROMPTS_FILE_PATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: '{prompts_file}'")
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {prompts_file}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ {prompts_file}: {e}")
            raise

    def _initialize_rag_chain(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ RAG-—Ü–µ–ø–æ—á–∫–∏...")

        persist_directory = os.getenv("PERSIST_DIRECTORY")
        if not persist_directory:
             raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PERSIST_DIRECTORY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –≤ '{persist_directory}'...")
        embeddings = OpenAIEmbeddings(chunk_size=1000)
        self.db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        self.retriever = self.db.as_retriever(search_type="mmr", search_kwargs={"k": 5})
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ.")

        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["contextualize_q_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        history_aware_retriever = create_history_aware_retriever(self.llm, self.retriever, contextualize_q_prompt)

        qa_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)

        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        self.conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        logger.info("--- RAG-—Ü–µ–ø–æ—á–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞ ---")

    def reload(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ RAG-—Ü–µ–ø–æ—á–∫—É."""
        logger.info("üîÉ –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É –∞–≥–µ–Ω—Ç–∞...")
        try:
            self.prompts = self.load_prompts()
            self._initialize_rag_chain()
            logger.info("‚úÖ –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω —Å –Ω–æ–≤–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –∞–≥–µ–Ω—Ç–∞: {e}", exc_info=True)
            return False

    def get_session_history(self, session_id: str):
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    def get_response_generator(self, user_question: str, session_id: str):
        stream = self.conversational_rag_chain.stream(
            {"input": user_question},
            config={"configurable": {"session_id": session_id}},
        )
        for chunk in stream:
            if 'answer' in chunk:
                yield chunk['answer']
