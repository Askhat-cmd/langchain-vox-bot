from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging
import json
import os

load_dotenv()

logger = logging.getLogger(__name__)

class Agent:
    def __init__(self) -> None:
        logger.info("--- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ê–≥–µ–Ω—Ç–∞ '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' ---")
        self.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.2, streaming=True)
        self.store = {}
        self.last_kb = "general"
        try:
            self.prompts = self.load_prompts()
        except (ValueError, FileNotFoundError, json.JSONDecodeError) as e:
            logger.critical(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤. {e}", exc_info=True)
            # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∑–∞–ø—É—Å–∫ —Å –Ω–µ–≤–µ—Ä–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            raise SystemExit(f"–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {e}")

        self._initialize_rag_chain()
        logger.info("--- –ê–≥–µ–Ω—Ç '–ú–µ—Ç—Ä–æ—Ç–µ—Å—Ç' —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ---")

    def load_prompts(self):
        prompts_file = os.getenv("PROMPTS_FILE_PATH")
        if not prompts_file:
            raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PROMPTS_FILE_PATH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: '{prompts_file}'")
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {prompts_file}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ {prompts_file}: {e}")
            raise

    def _initialize_rag_chain(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG."""
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ RAG-—Ü–µ–ø–æ—á–∫–∏...")

        persist_directory = os.getenv("PERSIST_DIRECTORY")
        if not persist_directory:
             raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è PERSIST_DIRECTORY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

        logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –≤ '{persist_directory}'...")
        embeddings = OpenAIEmbeddings(chunk_size=1000)
        self.db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
        # –î–≤–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º, –º–µ–Ω—å—à–∏–π k –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        kb_k = int(os.getenv("KB_TOP_K", "3"))
        self.retriever_general = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "general"}}
        )
        self.retriever_tech = self.db.as_retriever(
            search_type="similarity", search_kwargs={"k": kb_k, "filter": {"kb": "tech"}}
        )
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ.")

        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["contextualize_q_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        # –ò—Å—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã –¥–ª—è –æ–±–µ–∏—Ö –ë–ó
        history_aware_retriever_general = create_history_aware_retriever(self.llm, self.retriever_general, contextualize_q_prompt)
        history_aware_retriever_tech = create_history_aware_retriever(self.llm, self.retriever_tech, contextualize_q_prompt)

        qa_prompt = ChatPromptTemplate.from_messages(
            [("system", self.prompts["qa_system_prompt"]), MessagesPlaceholder("chat_history"), ("human", "{input}")]
        )
        question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)

        rag_chain_general = create_retrieval_chain(history_aware_retriever_general, question_answer_chain)
        rag_chain_tech = create_retrieval_chain(history_aware_retriever_tech, question_answer_chain)

        self.conversational_rag_chain_general = RunnableWithMessageHistory(
            rag_chain_general,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        self.conversational_rag_chain_tech = RunnableWithMessageHistory(
            rag_chain_tech,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer",
        )
        logger.info("--- RAG-—Ü–µ–ø–æ—á–∫–∞ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞ ---")

    def _route_kb(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è: general | tech.
        –í—ã–±–∏—Ä–∞–µ–º 'tech', –µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã; –∏–Ω–∞—á–µ 'general'."""
        if not text:
            return "general"
        t = text.lower()
        tech_markers = [
            "—Ç–µ—Ö–Ω", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç", "–ø–∞—Ä–∞–º–µ—Ç—Ä", "–¥–∏–∞–ø–∞–∑–æ–Ω", "–∫–ª–∞—Å—Å —Ç–æ—á–Ω–æ—Å—Ç–∏", "–Ω–∞–≥—Ä—É–∑", "–¥–∞—Ç—á–∏–∫",
            "—Ç–µ–Ω–∑–æ", "—Ä–∞–∑—Ä—ã–≤–Ω", "–º–∞—à–∏–Ω", "—É—Å–∏–ª–∏–µ", "–∫–Ω", "–º–ø–∞", "–Ω—å—é—Ç–æ–Ω", "–º–º", "–≥–æ—Å—Ç", "iso",
            "—Å–µ—Ä—Ç–∏—Ñ", "–º–æ–¥—É–ª—å", "—á–∞—Å—Ç–æ—Ç–∞", "–≤–∏–±—Ä–æ", "—Å–æ–ø—Ä–æ—Ç–∏–≤–ª", "–ø—Ä–æ—Ç–æ–∫–æ–ª", "datasheet", "spec"
        ]
        general_markers = ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–∫–æ–Ω—Ç–∞–∫—Ç", "–≥–∞—Ä–∞–Ω—Ç", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–ø–ª–∞—Ç–∞", "–∞–¥—Ä–µ—Å"]
        if any(m in t for m in tech_markers) and not any(m in t for m in general_markers):
            return "tech"
        return "general"

    def _max_relevance(self, question: str, kb: str, k: int) -> float:
        try:
            # –î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ—Ä–æ–≥–∞ –±–µ—Ä—ë–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ç–æ—Ä–∞
            res = self.db.similarity_search_with_relevance_scores(
                question, k=k, filter={"kb": kb}
            )
            if not res:
                return 0.0
            return max(score for _, score in res)
        except Exception:
            return 1.0  # –µ—Å–ª–∏ —Å—Ç–æ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ü–µ–Ω–∫—É, –Ω–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏–º —Ñ–æ–ª–±—ç–∫

    def reload(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç—ã, –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ RAG-—Ü–µ–ø–æ—á–∫—É."""
        logger.info("üîÉ –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫—É –∞–≥–µ–Ω—Ç–∞...")
        try:
            self.prompts = self.load_prompts()
            self._initialize_rag_chain()
            logger.info("‚úÖ –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω —Å –Ω–æ–≤–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏.")
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –∞–≥–µ–Ω—Ç–∞: {e}", exc_info=True)
            return False

    def get_session_history(self, session_id: str):
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]

    def get_response_generator(self, user_question: str, session_id: str):
        target = self._route_kb(user_question)
        # –ü–æ—Ä–æ–≥ –∏ k –±–µ—Ä—ë–º –∏–∑ env (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2 –∏ 3)
        threshold = float(os.getenv("KB_FALLBACK_THRESHOLD", "0.2"))
        k = int(os.getenv("KB_TOP_K", "3"))
        # –û—Ü–µ–Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ë–ó; –ø—Ä–∏ –Ω–∏–∑–∫–æ–π ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º –≤—Ç–æ—Ä—É—é
        max_score = self._max_relevance(user_question, target, k)
        alt = "tech" if target == "general" else "general"
        if max_score < threshold:
            alt_score = self._max_relevance(user_question, alt, k)
            if alt_score > max_score:
                logger.info(
                    f"–ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å ({max_score:.2f}) –¥–ª—è {target} ‚Üí –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ {alt} ({alt_score:.2f})"
                )
                target = alt
        self.last_kb = target
        logger.info(f"–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –≤ –ë–ó: {target}")
        chain = self.conversational_rag_chain_tech if target == "tech" else self.conversational_rag_chain_general
        stream = chain.stream(
            {"input": user_question},
            config={"configurable": {"session_id": session_id}},
        )
        for chunk in stream:
            if 'answer' in chunk:
                yield chunk['answer']
